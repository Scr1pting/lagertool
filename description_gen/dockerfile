# Use a lightweight Python image
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# Set working directory
WORKDIR /app

# Copy requirements first (to leverage Docker cache)
COPY requirements.txt .

# Install system dependencies needed for llama-cpp-python
RUN apt-get update && apt-get install -y \
        build-essential \
        cmake \
        git \
        wget \
        curl \
        libopenblas-dev \
    && pip install --upgrade pip setuptools wheel \
    && pip install -r requirements.txt \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Create models directory and download model using huggingface-hub
RUN pip install huggingface-hub \
    && mkdir -p ./models \
    && python -c "from huggingface_hub import hf_hub_download; \
    hf_hub_download( \
        repo_id='ggml-org/gemma-3-1b-it-GGUF', \
        filename='gemma-3-1b-it-Q4_K_M.gguf', \
        local_dir='./models' \
    )"

# Copy app code
COPY ./app ./app

# Expose FastAPI port
EXPOSE 8080

# Run the API
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
